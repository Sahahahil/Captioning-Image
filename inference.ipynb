{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34f4207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "from model import EncoderCNN, DecoderRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77bcda43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(checkpoint_dir, vocab_path, embed_size=256, hidden_size=512, device=\"cuda\"):\n",
    "    \"\"\"Load trained encoder, decoder, and vocabulary\"\"\"\n",
    "    encoder_path = os.path.join(checkpoint_dir, \"best_encoder.pth\")\n",
    "    decoder_path = os.path.join(checkpoint_dir, \"best_decoder.pth\")\n",
    "\n",
    "\n",
    "    if not os.path.exists(vocab_path):\n",
    "        raise FileNotFoundError(f\"‚ùå Vocabulary file not found: {vocab_path}\")\n",
    "    if not os.path.exists(encoder_path):\n",
    "        raise FileNotFoundError(f\"‚ùå Encoder checkpoint not found: {encoder_path}\")\n",
    "    if not os.path.exists(decoder_path):\n",
    "        raise FileNotFoundError(f\"‚ùå Decoder checkpoint not found: {decoder_path}\")\n",
    "\n",
    "\n",
    "    print(f\"üìö Loading vocabulary from: {vocab_path}\")\n",
    "    with open(vocab_path, \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "\n",
    "    print(f\"üß† Vocabulary size: {len(vocab)}\")\n",
    "    print(f\"üèóÔ∏è Initializing models...\")\n",
    "\n",
    "\n",
    "    encoder = EncoderCNN(embed_size).to(device)\n",
    "    decoder = DecoderRNN(embed_size, hidden_size, len(vocab)).to(device)\n",
    "\n",
    "\n",
    "    print(f\"‚ö° Loading model weights...\")\n",
    "    encoder.load_state_dict(torch.load(encoder_path, map_location=device))\n",
    "    # Load decoder state dict, ignoring unexpected keys\n",
    "    state_dict = torch.load(decoder_path, map_location=device)\n",
    "    state_dict = {k: v for k, v in state_dict.items() if k in decoder.state_dict()}\n",
    "    decoder.load_state_dict(state_dict)\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "\n",
    "    print(f\"‚úÖ Models loaded successfully!\")\n",
    "    return encoder, decoder, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b59f1c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"‚ùå Image file not found: {image_path}\")\n",
    "\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        processed = transform(image).unsqueeze(0) # add batch dimension\n",
    "        print(f\"üñºÔ∏è Image preprocessed: {image.size} -> {processed.shape}\")\n",
    "        return processed\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"‚ùå Error processing image {image_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9e9107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path, encoder, decoder, vocab, device=\"cuda\", max_length=50, use_beam=True):\n",
    "    image = preprocess_image(image_path).to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = encoder(image)\n",
    "        print(f\"üîç Image features shape: {features.shape}\")\n",
    "\n",
    "\n",
    "    if use_beam and hasattr(decoder, 'sample'):\n",
    "        caption = decoder.sample(features, vocab, max_len=max_length)[0] # take first in batch\n",
    "    else:\n",
    "        caption = decoder.greedy_sample(features, vocab, max_len=max_length)[0]\n",
    "\n",
    "\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"üöÄ Image Caption Generator\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "    image_path = \"/home/sahil_duwal/Projects/ImageCap/flickr8k/images/3744832122_2f4febdff6.jpg\"\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "    vocab_path = os.path.join(checkpoint_dir, \"vocab.pkl\")\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        encoder, decoder, vocab = load_models(checkpoint_dir, vocab_path, device=device)\n",
    "        print(f\"\\nüîÑ Generating caption for: {image_path}\")\n",
    "        caption = generate_caption(image_path, encoder, decoder, vocab, device=device, max_length=50)\n",
    "\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"üñºÔ∏è Image: {os.path.basename(image_path)}\")\n",
    "        print(f\"üìù Generated Caption: {caption}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        print(\"\\nüí° Make sure you have:\")\n",
    "        print(\" 1. Trained your model (run train.py)\")\n",
    "        print(\" 2. Correct checkpoint directory path\")\n",
    "        print(\" 3. Valid image file path\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during inference: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b77394f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Image Caption Generator\n",
      "==================================================\n",
      "üñ•Ô∏è Using device: cuda\n",
      "üìö Loading vocabulary from: checkpoints/vocab.pkl\n",
      "üß† Vocabulary size: 2994\n",
      "üèóÔ∏è Initializing models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahil_duwal/Projects/Vision/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sahil_duwal/Projects/Vision/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Loading model weights...\n",
      "‚úÖ Models loaded successfully!\n",
      "\n",
      "üîÑ Generating caption for: /home/sahil_duwal/Projects/ImageCap/flickr8k/images/10815824_2997e03d76.jpg\n",
      "üñºÔ∏è Image preprocessed: (500, 333) -> torch.Size([1, 3, 224, 224])\n",
      "üîç Image features shape: torch.Size([1, 256])\n",
      "\n",
      "==================================================\n",
      "üñºÔ∏è Image: 10815824_2997e03d76.jpg\n",
      "üìù Generated Caption: many people sit on a bench .\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
